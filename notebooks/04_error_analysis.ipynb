{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93375111",
   "metadata": {},
   "source": [
    "## Week 3–4 Misclassification Review\n",
    "This section summarises post-tuning error patterns so we can target threshold calibration and feature review:\n",
    "- LogisticRegression_Tuned, RandomForest_Tuned, and XGBoost_Tuned now concentrate the majority of their mistakes as **false positives (≈87–88%)**, reflecting the recall-first configuration.\n",
    "- NeuralNetwork_Tuned flips the baseline tendency: it keeps false negatives low (≈6%) but creates many false positives (≈94%), confirming the need for Week 5–6 threshold calibration before Gradio deployment.\n",
    "- High false-positive rows for NeuralNetwork_Tuned are dominated by low self-reported health (`numeric__health`), high perceived effort (`numeric__flteeff`), and reduced sleep/rest (`numeric__slprl`). False negatives skew toward high enjoyment of life and frequent sports, a cue to inspect feature scaling and interaction terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b182af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error proportions by model (False Negative vs False Positive):\n",
      "error_type                 False Negative  False Positive\n",
      "model                                                    \n",
      "logistic_regression                 0.126           0.874\n",
      "logistic_regression_tuned           0.126           0.874\n",
      "neural_network                      0.922           0.078\n",
      "neural_network_tuned                0.061           0.939\n",
      "random_forest                       0.865           0.135\n",
      "random_forest_tuned                 0.130           0.870\n",
      "xgboost                             0.910           0.090\n",
      "xgboost_tuned                       0.120           0.880\n",
      "NeuralNetwork_Tuned — top numeric averages for false negatives:\n",
      "numeric__inprdsc    0.368\n",
      "numeric__enjlf      0.323\n",
      "numeric__dosprt     0.227\n",
      "numeric__happy      0.163\n",
      "numeric__wrhpp      0.159\n",
      "dtype: float64\n",
      "NeuralNetwork_Tuned — top numeric averages for false positives:\n",
      "numeric__health     0.754\n",
      "numeric__flteeff    0.320\n",
      "numeric__slprl      0.314\n",
      "numeric__fltdpr     0.291\n",
      "numeric__fltsd      0.260\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "misclassified_path = PROJECT_ROOT / 'results' / 'metrics' / 'misclassified_samples.csv'\n",
    "if not misclassified_path.exists():\n",
    "    raise FileNotFoundError('Expected misclassified_samples.csv after evaluation step.')\n",
    "\n",
    "mis = pd.read_csv(misclassified_path)\n",
    "summary = mis.groupby('model')['error_type'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "print('Error proportions by model (False Negative vs False Positive):')\n",
    "print(summary.round(3))\n",
    "\n",
    "nn_tuned = mis[mis['model'] == 'neural_network_tuned'].copy()\n",
    "if not nn_tuned.empty:\n",
    "    numeric_cols = [c for c in nn_tuned.columns if c.startswith('numeric__')]\n",
    "    fn_means = nn_tuned[nn_tuned['error_type'] == 'False Negative'][numeric_cols].mean().sort_values(ascending=False).head(5)\n",
    "    fp_means = nn_tuned[nn_tuned['error_type'] == 'False Positive'][numeric_cols].mean().sort_values(ascending=False).head(5)\n",
    "    print('\n",
    "NeuralNetwork_Tuned — top numeric averages for false negatives:')\n",
    "    print(fn_means.round(3))\n",
    "    print('\n",
    "NeuralNetwork_Tuned — top numeric averages for false positives:')\n",
    "    print(fp_means.round(3))\n",
    "else:\n",
    "    print('No neural_network_tuned records found; rerun evaluation if this is unexpected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399abf79",
   "metadata": {},
   "source": [
    "### Threshold Calibration Sweep (Tuned Models)\n",
    "This section summarises how recall-focused models behave across probability thresholds (0.2–0.8). Metrics are saved to `results/metrics/threshold_sweep.csv` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bfdc84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data splits from /Users/peter/Desktop/AI_MLProjects_Research_Project/health_xai_project/results/models/data_splits.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peter/Desktop/AI_MLProjects_Research_Project/venv/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/peter/Desktop/AI_MLProjects_Research_Project/venv/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved threshold sweep metrics to /Users/peter/Desktop/AI_MLProjects_Research_Project/health_xai_project/results/metrics/threshold_sweep.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_regression_tuned</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.146393</td>\n",
       "      <td>0.970793</td>\n",
       "      <td>0.254420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic_regression_tuned</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.159309</td>\n",
       "      <td>0.948540</td>\n",
       "      <td>0.272800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic_regression_tuned</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.175751</td>\n",
       "      <td>0.919332</td>\n",
       "      <td>0.295089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic_regression_tuned</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.193004</td>\n",
       "      <td>0.874826</td>\n",
       "      <td>0.316239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>logistic_regression_tuned</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.215121</td>\n",
       "      <td>0.819193</td>\n",
       "      <td>0.340758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model  threshold  precision    recall  f1_score\n",
       "0  logistic_regression_tuned       0.20   0.146393  0.970793  0.254420\n",
       "1  logistic_regression_tuned       0.25   0.159309  0.948540  0.272800\n",
       "2  logistic_regression_tuned       0.30   0.175751  0.919332  0.295089\n",
       "3  logistic_regression_tuned       0.35   0.193004  0.874826  0.316239\n",
       "4  logistic_regression_tuned       0.40   0.215121  0.819193  0.340758"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "try:\n",
    "    from src.evaluate_models import load_splits, load_models\n",
    "except ModuleNotFoundError:\n",
    "    import os, sys\n",
    "    PROJECT_ROOT = Path('..').resolve()\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    from src.evaluate_models import load_splits, load_models\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "threshold_out = PROJECT_ROOT / 'results' / 'metrics' / 'threshold_sweep.csv'\n",
    "\n",
    "splits = load_splits()\n",
    "models, scaler = load_models(input_dim=splits['X_train'].shape[1], include_tuned=True)\n",
    "\n",
    "X_test = splits['X_test']\n",
    "y_test = splits['y_test'].astype(int)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_keys = ['logistic_regression_tuned', 'random_forest_tuned', 'xgboost_tuned', 'neural_network_tuned']\n",
    "thresholds = np.linspace(0.2, 0.8, 13)\n",
    "rows = []\n",
    "\n",
    "for key in model_keys:\n",
    "    if key not in models:\n",
    "        print(f\"Skipping {key} (model artefact not found)\")\n",
    "        continue\n",
    "    model = models[key]\n",
    "    features = X_test_scaled if key in {'logistic_regression_tuned', 'neural_network_tuned'} else X_test.values\n",
    "\n",
    "    if key == 'neural_network_tuned':\n",
    "        import torch\n",
    "        with torch.no_grad():\n",
    "            logits = model(torch.tensor(features, dtype=torch.float32))\n",
    "            scores = torch.sigmoid(logits).numpy().ravel()\n",
    "    else:\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            scores = model.predict_proba(features)[:, 1]\n",
    "        else:\n",
    "            scores = model.decision_function(features)\n",
    "            scores = 1 / (1 + np.exp(-scores))\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        preds = (scores >= thresh).astype(int)\n",
    "        rows.append({\n",
    "            'model': key,\n",
    "            'threshold': round(float(thresh), 3),\n",
    "            'precision': precision_score(y_test, preds, zero_division=0),\n",
    "            'recall': recall_score(y_test, preds, zero_division=0),\n",
    "            'f1_score': f1_score(y_test, preds, zero_division=0),\n",
    "        })\n",
    "\n",
    "threshold_df = pd.DataFrame(rows)\n",
    "threshold_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "threshold_df.to_csv(threshold_out, index=False)\n",
    "print('Saved threshold sweep metrics to', threshold_out)\n",
    "display(threshold_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6460e7cd",
   "metadata": {},
   "source": [
    "### Recommended Thresholds (Max F1)\n",
    "Selecting the threshold with the highest F1 score per tuned model provides a balanced starting point for Week 5–6 calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ffe52df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       model  threshold  precision    recall  f1_score\n",
      "0  logistic_regression_tuned       0.65   0.322667  0.504868  0.393709\n",
      "1       neural_network_tuned       0.65   0.302987  0.592490  0.400941\n",
      "2        random_forest_tuned       0.60   0.327812  0.522949  0.403001\n",
      "3              xgboost_tuned       0.65   0.330508  0.542420  0.410742\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "threshold_df = pd.read_csv(Path('..') / 'results' / 'metrics' / 'threshold_sweep.csv')\n",
    "best_thresholds = (\n",
    "    threshold_df.loc[threshold_df.groupby('model')['f1_score'].idxmax()]\n",
    "    .sort_values('model')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(best_thresholds)\n",
    "best_thresholds.to_csv(Path('..') / 'results' / 'metrics' / 'threshold_recommendations.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6faa498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
